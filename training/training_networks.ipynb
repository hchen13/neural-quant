{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29227cfe-4331-438a-ab4e-c37211c020d2",
   "metadata": {},
   "source": [
    "# Training Networks\n",
    "\n",
    "## 0. Recap\n",
    "\n",
    "From [dataset creation](../dataset_management/dataset_generation.ipynb) and [building neural network](../prototype/define_model.ipynb) sections we have built a network and a dataset to train. It is time to actually training the model.\n",
    "\n",
    "\n",
    "## 1. Load and pre-process dataset\n",
    "\n",
    "Load the dataset using the tools we have built so far. To speed-up the training process, usually we need to batch the training data, and possibly pre-fetch the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3376b19-dce7-45a8-9eb2-80c129122a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "[INFO] 42300 files founded.\n",
      "[INFO] 4701 files founded.\n",
      "history shape: (32, 144, 5)\n",
      "label shape: (32,)\n",
      "future shape: (32, 12, 5)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "_project_root = Path().cwd().parent\n",
    "sys.path.append(str(_project_root))\n",
    "\n",
    "from dataset_management.tools import load_dataset\n",
    "from tensorflow.python.data.ops.dataset_ops import AUTOTUNE\n",
    "\n",
    "dataset_dir = _project_root / 'dataset'\n",
    "train_dir = dataset_dir / 'train'\n",
    "valid_dir = dataset_dir / 'valid'\n",
    "\n",
    "trainset = load_dataset(train_dir)\n",
    "validset = load_dataset(valid_dir)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "trainset = trainset.batch(batch_size).prefetch(AUTOTUNE)\n",
    "validset = validset.batch(batch_size).prefetch(AUTOTUNE)\n",
    "\n",
    "for (history, y, future) in trainset:\n",
    "    print(f\"history shape: {history.shape}\")\n",
    "    print(f\"label shape: {y.shape}\")\n",
    "    print(f\"future shape: {future.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920a1c0c-401c-45aa-8957-656e564d8d58",
   "metadata": {},
   "source": [
    "## 2. Create model\n",
    "\n",
    "Create a model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd741e16-3ea9-42cd-8b2a-53ee168925f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"FCN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 144, 5)]          0         \n",
      "_________________________________________________________________\n",
      "conv1_1 (ConvBn)             (None, 144, 8)            312       \n",
      "_________________________________________________________________\n",
      "conv1_2 (ConvBn)             (None, 144, 16)           448       \n",
      "_________________________________________________________________\n",
      "pool1_1 (MaxPooling1D)       (None, 72, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv2_1 (ConvBn)             (None, 72, 16)            832       \n",
      "_________________________________________________________________\n",
      "conv2_2 (ConvBn)             (None, 72, 32)            1664      \n",
      "_________________________________________________________________\n",
      "conv3_1 (Inception)          (None, 36, 64)            41888     \n",
      "_________________________________________________________________\n",
      "conv3_2 (Inception)          (None, 36, 64)            30208     \n",
      "_________________________________________________________________\n",
      "conv4_1 (Inception)          (None, 18, 128)           96064     \n",
      "_________________________________________________________________\n",
      "conv4_2 (Inception)          (None, 18, 128)           71168     \n",
      "_________________________________________________________________\n",
      "conv5_1 (Inception)          (None, 9, 256)            273216    \n",
      "_________________________________________________________________\n",
      "conv5_2 (Inception)          (None, 9, 256)            213696    \n",
      "_________________________________________________________________\n",
      "conv6_1 (Inception)          (None, 5, 384)            457984    \n",
      "_________________________________________________________________\n",
      "conv6_2pre (Inception)       (None, 5, 384)            302208    \n",
      "_________________________________________________________________\n",
      "conv6_2bn (BatchNormalizatio (None, 5, 384)            1152      \n",
      "_________________________________________________________________\n",
      "conv6_2 (ReLU)               (None, 5, 384)            0         \n",
      "_________________________________________________________________\n",
      "features (GlobalAveragePooli (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "prediction (Dense)           (None, 3)                 1155      \n",
      "=================================================================\n",
      "Total params: 1,491,995\n",
      "Trainable params: 1,481,419\n",
      "Non-trainable params: 10,576\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from prototype.fcn import build_fcn\n",
    "\n",
    "\n",
    "fcn = build_fcn(input_size=144)\n",
    "fcn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6e37d8-3a9a-4476-bab0-887a54e37242",
   "metadata": {},
   "source": [
    "## 3. Monitor the training\n",
    "\n",
    "During training, we often would like to see the various training outcomes and metrics as it goes to identify problems as quickly as possible. To make this happen, we would make use of the TensorBoard tool. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2f22a78-39a8-4684-af64-ae762accd104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "from settings import PROJECT_ROOT\n",
    "\n",
    "\n",
    "class Monitor:\n",
    "    def __init__(self, caption):\n",
    "        log_root = PROJECT_ROOT / 'training' / 'logs'\n",
    "        fullpath = log_root / caption\n",
    "        try:\n",
    "            shutil.rmtree(str(fullpath))\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "        fullpath.mkdir(exist_ok=True, parents=True)\n",
    "        self.logdir = fullpath\n",
    "        self.caption = caption\n",
    "        train_path = fullpath / 'train'\n",
    "        valid_path = fullpath / 'valid'\n",
    "        self.train_writer = tf.summary.create_file_writer(str(train_path))\n",
    "        self.valid_writer = tf.summary.create_file_writer(str(valid_path))\n",
    "\n",
    "    def scalar(self, tag, value, step):\n",
    "        if tag.startswith('train_'):\n",
    "            writer = self.train_writer\n",
    "            tag = tag[len('train_'):]\n",
    "        else:\n",
    "            writer = self.valid_writer\n",
    "            if tag.startswith('valid_'):\n",
    "                tag = tag[len('valid_'):]\n",
    "        with writer.as_default():\n",
    "            tf.summary.scalar(tag, data=value, step=step)\n",
    "\n",
    "    def write_reports(self, results, step, prefix=None):\n",
    "        tags = results\n",
    "        if prefix is not None:\n",
    "            tags = { f\"{prefix}{k}\": v for k, v in results.items() }\n",
    "        for key, val in tags.items():\n",
    "            self.scalar(key, val, step)\n",
    "\n",
    "    def graph(self, model):\n",
    "        from tensorflow.python.ops import summary_ops_v2\n",
    "        from tensorflow.python.keras import backend as K\n",
    "\n",
    "        with self.train_writer.as_default():\n",
    "            with summary_ops_v2.always_record_summaries():\n",
    "                if not model.run_eagerly:\n",
    "                    summary_ops_v2.graph(K.get_graph(), step=0)\n",
    "\n",
    "\n",
    "experiment_name = f\"FCN@{datetime.now().strftime('%-y%m%d-%H:%M:%S')}\"\n",
    "monitor = Monitor(experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b502dcf-ba40-4ddb-bb24-2fba5bbc9313",
   "metadata": {},
   "source": [
    "## 4. Set up optimizer\n",
    "\n",
    "Next up, create the model optimizer with one of the learning algorithms (SGD, Adam, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6707acce-ccac-4c74-b4bf-66725abcc8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db3cd04-63b6-4d23-bad1-f4f3415ec86b",
   "metadata": {},
   "source": [
    "## 5. Train\n",
    "\n",
    "Before actually getting into training, there's one more step which is to define how the network processes each batch of the data, including transforming the label data (integer) into one-hot vectors; balancing the losses because the data distribution is skewed; and updating the weights using the optimizer, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc94c8b-6c9b-4db1-a6bf-7d00fb9976d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_batch(model, batch_data, optimizer):\n",
    "    history, label, _ = batch_data\n",
    "    y_true = tf.one_hot(label + 1, depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b052ec7-7584-4593-81ef-dc5ed608b118",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "global_step = 0\n",
    "for e in range(epochs):\n",
    "    print(f\"epoch #{e + 1}/{epochs} @{datetime.now()}:\")\n",
    "    for local_step, batch_data in enumerate(trainset):\n",
    "        global_step += 1\n",
    "        train_report = train_on_batch(fcn, batch_data, opt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
